{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6a1d16-c7b5-4335-8769-57577d3d3702",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/vchabaux/recherche_images_gallica/blob/main/recherche_images_gallica.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c243f899-3c1e-4556-b089-0d11eae3f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db8421b-c0bd-4c54-93bc-6a7ca33ca75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:import gradio as gr\n",
    "except:\n",
    "  %pip install gradio\n",
    "  import gradio as gr\n",
    "\n",
    "try:import xmltodict\n",
    "except:\n",
    "  %pip install xmltodict\n",
    "  import xmltodict\n",
    "\n",
    "try:from lavis.models import load_model_and_preprocess\n",
    "except:\n",
    "  %pip install salesforce-lavis\n",
    "  from lavis.models import load_model_and_preprocess\n",
    "\n",
    "try:from transformers import MarianMTModel, MarianTokenizer\n",
    "except:\n",
    "  %pip install transformers\n",
    "  from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "try:from sentence_transformers import SentenceTransformer, util\n",
    "except:\n",
    "  %pip install sentence-transformers\n",
    "  from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import csv\n",
    "from math import dist\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "from urllib.error import HTTPError, URLError\n",
    "import requests\n",
    "import torch\n",
    "from torch import tensor\n",
    "import time\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Dossier du corpus d'image (sera créé au m^me niveau que ce notebook)\n",
    "DIR_BASE = \"gdrive/MyDrive/recherche_images_gallica/IMG_CORPUS\" \n",
    "# Dossier des fichiers d'embeddings  (sera créé au même niveau que ce notebook)\n",
    "DIR_INDEX = \"gdrive/MyDrive/recherche_images_gallica/IMG_INDEXS\" \n",
    "# Dossier temporaire de la collecte  (sera créé au même niveau que ce notebook)\n",
    "DIR_TMP = \"gdrive/MyDrive/recherche_images_gallica/IMG_TMP\" \n",
    "\n",
    "# Initialisation de Blip2 (lavis framework)\n",
    "model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip2_feature_extractor\", model_type=\"coco\", device=device)\n",
    "\n",
    "# Initialisation du transformer encoder-decoders pour la traduction (Opus-MT based on Marian-NMT)\n",
    "model_name = 'Helsinki-NLP/opus-mt-fr-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "marian = MarianMTModel.from_pretrained(model_name)\n",
    "marian = marian.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1254b03d-98a1-4517-ba78-af7ff6048533",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Retourne le nombre de pages du document)\n",
    "def nombre_pages(ark):\n",
    "  # In : identifiant ark | Out : nombre de pages (int)\n",
    "  PAGINATION_BASEURL = 'https://gallica.bnf.fr/services/Pagination?ark='\n",
    "  url = \"\".join([PAGINATION_BASEURL, ark])\n",
    "  s = requests.get(url, stream=True)\n",
    "  paginationdic = xmltodict.parse(s.text)\n",
    "  nb_pages = int(paginationdic[\"livre\"][\"structure\"][\"nbVueImages\"])\n",
    "  return nb_pages\n",
    "\n",
    "\n",
    "#Calcul la distance entre le milieu-bas du rectangle superieur (image) et le milieu-haut du rectangle inferieur(légende)\n",
    "def rect_distance(rect1, rect2):\n",
    "    x1, y1, x1b, y1b = rect1\n",
    "    x2, y2, x2b, y2b = rect2\n",
    "    # Coordonnées des milieux des côtés\n",
    "    milieu_haut_rect2 = ((x2 + x2b) / 2, y2)\n",
    "    milieu_bas_rect1 = ((x1 + x1b) / 2, y1b)   \n",
    "    # Calcul de la distance entre les milieux\n",
    "    distance = dist(milieu_haut_rect2, milieu_bas_rect1)     \n",
    "    return distance\n",
    "\n",
    "def translate_legend(row):\n",
    "    if row[\"legend\"] is not np.nan and row[\"legend\"] is not None and row[\"legend\"].strip() != \"\":\n",
    "        inputs = tokenizer.encode(row[\"legend\"], return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        outputs = marian.generate(inputs, num_beams=4, max_length=200, early_stopping=True)\n",
    "        translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(row[\"legend\"], translated_text)\n",
    "        return translated_text\n",
    "    else:\n",
    "        print(row[\"legend\"], \"\")\n",
    "        return \"\"\n",
    "\n",
    "# Retourne les embeddings\n",
    "def get_embedding(row):\n",
    "    img_path = row[\"img_path\"]\n",
    "    txt = row[\"en_legend\"] if row[\"en_legend\"] is not np.nan else \"\"\n",
    "\n",
    "    # CHargement et pre-process de l'image\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image_processed = vis_processors[\"eval\"](image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # pre-process de la légende (en anglais)\n",
    "    text_input = txt_processors[\"eval\"](txt)\n",
    "    \n",
    "    sample = {\"image\": image_processed, \"text_input\": text_input}\n",
    "\n",
    "    # English embedding\n",
    "    text_emb = model.extract_features(sample, mode=\"text\").text_embeds_proj[:,0,:] # size (1, 256)\n",
    "    text_emb /= text_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Image embedding\n",
    "    image_emb = model.extract_features(sample, mode=\"image\").image_embeds_proj[:,0,:] # size (1, 256)\n",
    "    image_emb /= image_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # mean embedding\n",
    "    mean_emb = torch.stack([image_emb, text_emb]).mean(dim=0)\n",
    "\n",
    "    print(\"Embedding :\",img_path, \"\\t\", txt)\n",
    "    \n",
    "    # Retourne l'embedding du texte, l'embedding de l'image et l'embedding moyen (image et texte) # size : (1,256)\n",
    "    return [text_emb, image_emb, mean_emb]\n",
    "\n",
    "\n",
    "# Variable globale (sry) représentant le csv d'embedding (corpus) actuellement selectionné\n",
    "base = None\n",
    "\n",
    "\n",
    "# Charger le csv d'embedding selectionné dans la liste déroulante\n",
    "def load_csv(corpus_select):\n",
    "    global DIR_INDEX\n",
    "    global base\n",
    "    print(corpus_select)\n",
    "    base = pd.read_csv(DIR_INDEX+\"/\"+corpus_select)\n",
    "    base['embedding'] = base['embedding'].apply(lambda x: eval(x.replace('\"cuda:0\"', \"device\"))) \n",
    "\n",
    "\n",
    "# Lance la collecte et l'embedding d'une liste d'arks sous forme de string(\"a,b,c\") avec un nom de collection coll_name\n",
    "def search_ark_fn(search_ark, coll_name):\n",
    "    global DIR_BASE\n",
    "    global DIR_INDEX\n",
    "    global DIR_TMP\n",
    "    \n",
    "    # Parse de la liste de d'arks en entrée\n",
    "    arks=search_ark.split(\",\")\n",
    "\n",
    "    # Création des dossiers s'il n'existent pas\n",
    "    try:\n",
    "        os.makedirs(DIR_BASE)        \n",
    "    except FileExistsError:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        os.makedirs(DIR_TMP)        \n",
    "    except FileExistsError:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        os.makedirs(DIR_BASE+\"/\"+coll_name)        \n",
    "    except:\n",
    "        return \"Erreur : ce nom de collection exite déjà ou n'est pas valide !\"\n",
    "        \n",
    "    try:\n",
    "        os.makedirs(DIR_INDEX)        \n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    # Liste des images collectées\n",
    "    base_img = []\n",
    "    if os.path.exists(DIR_TMP+\"/\"+coll_name+\".csv\"):\n",
    "        os.remove(DIR_TMP+\"/\"+coll_name+\".csv\")\n",
    "    with open(DIR_TMP+\"/\"+coll_name+\".csv\", \"a\", encoding='utf-8') as tmp_file:\n",
    "        csv_writer = csv.writer(tmp_file)\n",
    "        for ark in arks:\n",
    "            links = {}\n",
    "            pages = nombre_pages(ark)\n",
    "            print(\"\\nARK :\", ark)\n",
    "            \n",
    "            for page in range(1, pages+1) : \n",
    "                images = []\n",
    "                texts = []\n",
    "                alto_url = 'https://gallica.bnf.fr/RequestDigitalElement?O={}&E=ALTO&Deb={}'.format(ark, page) \n",
    "                # Boucle de requête de l'alto de la page. Si erreur sleep 15 secondes. Skip la page à la 3eme erreur\n",
    "                fail_counter = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        s = requests.get(alto_url, stream=True)\n",
    "                        break\n",
    "                    except:\n",
    "                        fail_counter += 1\n",
    "                        if fail_counter > 2:\n",
    "                            print(\"Echec de collecte de l'alto avec l'url :\", alto_url, \"Echecs:\", fail_counter)\n",
    "                            print(\"Top d'echecs, page is skiped\")    \n",
    "                            break\n",
    "                        else: \n",
    "                            print(\"Echec de collecte de l'alto avec l'url :\", alto_url, \"Echecs:\", fail_counter)\n",
    "                            print(\"Nouvel essai dans 15 secondes\")         \n",
    "                            time.sleep(15)\n",
    "                            continue                        \n",
    "                if fail_counter > 2:\n",
    "                    break\n",
    "                    \n",
    "                # Vérifier si la page est est océrisée, sinon la page est skiped\n",
    "                try:\n",
    "                    altodic = xmltodict.parse(s.text)\n",
    "                except : \n",
    "                    print(ark, \"Document non océrisé. Skiped\")\n",
    "                    break\n",
    "    \n",
    "                # Collecte des images et légendes \n",
    "                print(\"==========\", \"Page\", page,\"==========\")\n",
    "                cbs = altodic[\"alto\"][\"Layout\"][\"Page\"].get(\"PrintSpace\", {}).get(\"TextBlock\", [])\n",
    "                if not isinstance(cbs, list): cbs = [cbs]\n",
    "                for cb in cbs:\n",
    "                    content = []\n",
    "                    textLines = cb.get(\"TextLine\",[])\n",
    "                    if not isinstance(textLines, list): textLines = [textLines]\n",
    "                    for textLine in textLines:\n",
    "                        strings = textLine.get(\"String\",[])\n",
    "                        if not isinstance(strings, list): strings = [strings]\n",
    "                        content.extend(string.get(\"@CONTENT\") for string in strings)\n",
    "                    texts.append(((int(cb[\"@HPOS\"]), int(cb[\"@VPOS\"]), int(cb[\"@HPOS\"])+int(cb[\"@WIDTH\"]), int(cb[\"@VPOS\"])+int(cb[\"@HEIGHT\"])), \" \".join(content)))        \n",
    "                cbs = altodic[\"alto\"][\"Layout\"][\"Page\"].get(\"PrintSpace\", {}).get(\"Illustration\", [])\n",
    "                if not isinstance(cbs, list): cbs = [cbs]\n",
    "                for cb in cbs:        \n",
    "                    images.append(((int(cb[\"@HPOS\"]), int(cb[\"@VPOS\"]), int(cb[\"@HPOS\"])+int(cb[\"@WIDTH\"]), int(cb[\"@VPOS\"])+int(cb[\"@HEIGHT\"])), cb))\n",
    "                cbs = altodic[\"alto\"][\"Layout\"][\"Page\"].get(\"PrintSpace\", {}).get(\"ComposedBlock\", [])\n",
    "                if not isinstance(cbs, list): cbs = [cbs]\n",
    "                for cb in cbs:\n",
    "                    illustration = cb.get(\"Illustration\", [])\n",
    "                    if not isinstance(illustration, list): \n",
    "                        illustration = [illustration]\n",
    "                    textBlocks = cb.get(\"TextBlock\", [])\n",
    "                    if not isinstance(textBlocks, list): \n",
    "                        textBlocks = [textBlocks]                \n",
    "                    for cb in textBlocks:\n",
    "                        content = []\n",
    "                        textLines = cb.get(\"TextLine\",[])\n",
    "                        if not isinstance(textLines, list): \n",
    "                            textLines = [textLines]                    \n",
    "                        for textLine in textLines:\n",
    "                            strings = textLine.get(\"String\",[])\n",
    "                            if not isinstance(strings, list): strings = [strings]\n",
    "                            content.extend(string.get(\"@CONTENT\") for string in strings)                        \n",
    "                        texts.append(((int(cb[\"@HPOS\"]), int(cb[\"@VPOS\"]), int(cb[\"@HPOS\"])+int(cb[\"@WIDTH\"]), int(cb[\"@VPOS\"])+int(cb[\"@HEIGHT\"])), \" \".join(content)))                        \n",
    "                    for cb in illustration:\n",
    "                        images.append(((int(cb[\"@HPOS\"]), int(cb[\"@VPOS\"]), int(cb[\"@HPOS\"])+int(cb[\"@WIDTH\"]), int(cb[\"@VPOS\"])+int(cb[\"@HEIGHT\"])),cb))\n",
    "    \n",
    "                # Récupérer toutes les images (et leur légende) identifiées sur la page\n",
    "                for i,img in enumerate(images) :\n",
    "                    url = \"https://gallica.bnf.fr/iiif/ark:/12148/{}/f{}/{},{},{},{}/{}/0/native.jpg\".format(ark,page,img[1][\"@HPOS\"],img[1][\"@VPOS\"],img[1][\"@WIDTH\"],img[1][\"@HEIGHT\"],\"full\")\n",
    "                    nomfichier = ark+\"_\"+str(page)+\"_\"+img[1][\"@ID\"]+\".jpg\"\n",
    "                    cheminout = DIR_BASE+\"/\"+coll_name+\"/\"+nomfichier                \n",
    "    \n",
    "                    # Boucle de téléchargement de l'image. si erreur, réessaie dans 1à seconds\n",
    "                    while True:\n",
    "                        try :\n",
    "                            urllib.request.urlretrieve(url, cheminout)\n",
    "                        except (HTTPError, URLError) as erreur:\n",
    "                            print(str(erreur.reason))\n",
    "                            print(\"wait 10 seconds\")\n",
    "                            time.sleep(10)\n",
    "                        break\n",
    "                    \n",
    "                    try:\n",
    "                        # Identifier et récupérer la légende de l'image (si trouvée)\n",
    "                        txt_rank = []\n",
    "                        legend = []\n",
    "                        for txt in texts:\n",
    "                            distance = rect_distance(img[0], txt[0])\n",
    "                            if distance <100 : legend.append(txt[1])\n",
    "                            txt_rank.append((distance, txt[1]))\n",
    "                        txt_rank.sort(key= lambda x : x[0])\n",
    "                        txt_legned = \" \".join(legend)\n",
    "                        print(\"Image :\", i, \"| Description :\",txt_legned)\n",
    "                        #for rank in txt_rank:print(rank)\n",
    "    \n",
    "                        # Ajouter l'image à la liste\n",
    "                        #base_img.append((cheminout, txt_legned))\n",
    "                        csv_writer.writerow([cheminout, txt_legned])\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "    \n",
    "    # load la liste image<>legende dans un dataframe pandas\n",
    "    index_df = pd.read_csv(DIR_TMP+\"/\"+coll_name+\".csv\", names=[\"img_path\", \"legend\"], encoding=\"utf-8\")\n",
    "    \n",
    "    # Traduction des légendes en anglais (pour Blip2)\n",
    "    #legends = index_df[\"legend\"].fillna(\"\").tolist()\n",
    "    #translated = marian.generate(**tokenizer(legends, return_tensors=\"pt\", padding=True))\n",
    "    #en_legends = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "    #index_df[\"en_legend\"] = np.array(en_legends)\n",
    "    #index_df[\"en_legend\"][(index_df[\"legend\"].isna()) | (index_df[\"legend\"].str.strip() == \"\") | (index_df[\"legend\"] is None)] = \"\"    \n",
    "    index_df[\"en_legend\"] = index_df.apply(translate_legend, axis = 1)\n",
    "    \n",
    "    # Obtenir les embedding du corpus par Blip2\n",
    "    index_df[\"embedding\"] = index_df.apply(get_embedding, axis = 1)\n",
    "\n",
    "    # Sauvegarder le fichier csv d'embeddings\n",
    "    index_df.to_csv(DIR_INDEX+\"/\"+coll_name+\".csv\", index=False, encoding=\"utf-8\")  \n",
    "\n",
    "\n",
    "# Met à jour le corpu selectionné selon la selection de l'utilisateur\n",
    "def update_corpus():    \n",
    "    corpus = []    \n",
    "    for csv_file in os.listdir(DIR_INDEX):\n",
    "        if csv_file.lower().endswith('.csv'):\n",
    "            corpus.append(csv_file)\n",
    "    return gr.update(choices=corpus)\n",
    "\n",
    "\n",
    "# Deactivate UI\n",
    "def deactivate():\n",
    "    return gr.update(interactive=False), gr.update(interactive=False), gr.update(interactive=False), gr.update(interactive=False), gr.update(interactive=False), gr.update(interactive=False), gr.update(interactive=False), gr.update(interactive=False), gr.update(interactive=False)\n",
    "\n",
    "\n",
    "# Activate UI\n",
    "def activate():\n",
    "    return gr.update(interactive=True), gr.update(interactive=True), gr.update(interactive=True), gr.update(interactive=True), gr.update(interactive=True), gr.update(interactive=True), gr.update(interactive=True), gr.update(interactive=True), gr.update(interactive=True)\n",
    "\n",
    "\n",
    "# Lacer la recherche d'images similaires depuis l'UI\n",
    "def search(input_search_txt, input_search_type, img_count):\n",
    "    global base\n",
    "    # Calcul des similarités entre la recherche et et le type d'embedding choisi\n",
    "    def sim_calc(row, filter):\n",
    "        if filter == \"text_sim\":\n",
    "            sim = -1 if row[\"en_legend\"] is np.nan or row[\"en_legend\"] == \"\" else util.cos_sim(row[\"embedding\"][0], input_search_embedding).cpu().numpy()[0]\n",
    "        elif filter == \"image_sim\":\n",
    "            sim = util.cos_sim(row[\"embedding\"][1], input_search_embedding).cpu().numpy()[0]\n",
    "        else:\n",
    "            sim_mean = util.cos_sim(row[\"embedding\"][2], input_search_embedding).cpu().numpy()[0]\n",
    "            #sim_text = torch.matmul(row[\"embedding\"][0], input_search_embedding.T).cpu().numpy()[0]\n",
    "            sim_text = -1 if row[\"en_legend\"] is np.nan or row[\"en_legend\"] == \"\" else util.cos_sim(row[\"embedding\"][0], input_search_embedding).cpu().numpy()[0]\n",
    "            sim_image = util.cos_sim(row[\"embedding\"][1], input_search_embedding).cpu().numpy()[0]\n",
    "            #sim_image = torch.matmul(row[\"embedding\"][1], input_search_embedding.T).cpu().numpy()[0]\n",
    "            sim = (sim_mean + sim_text + sim_image)/3\n",
    "        return sim\n",
    "\n",
    "    # Traduction, Tokenization et Embedding de la l'entrée texte de recherche\n",
    "    inputs = tokenizer.encode(input_search_txt, return_tensors=\"pt\").to(device)\n",
    "    outputs = marian.generate(inputs, num_beams=4, max_length=50, early_stopping=True)\n",
    "    input_search_tk = tokenizer.decode(outputs[0], skip_special_tokens=True)   \n",
    "    input_search = txt_processors[\"eval\"](input_search_tk)\n",
    "    input_search_sample = {\"image\": None, \"text_input\": [input_search]}\n",
    "    input_search_embedding = model.extract_features(input_search_sample, mode=\"text\").text_embeds_proj[:,0,:] # size (1, 768)\n",
    "    input_search_embedding /= input_search_embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Get le type de similarité selectionnée\n",
    "    if input_search_type == \"Texte & Image\":\n",
    "        filter = \"total_mean\"\n",
    "    elif input_search_type == \"Image\":\n",
    "        filter = \"image_sim\"\n",
    "    else:\n",
    "        filter = \"text_sim\"   \n",
    "        \n",
    "    # Calcul des similarités  selon le filtre choisi : Text<>Texte, Texte<>Image ou Texte<>Mean(Texte,Image)\n",
    "    base['sim'] = base.apply(lambda x: sim_calc(x,filter), axis = 1)\n",
    "    \n",
    "    # Return images to gradio gallery\n",
    "    images = []\n",
    "    for index, row in base.sort_values(\"sim\", ascending=False).head(img_count).iterrows():\n",
    "        image = Image.open(\"gdrive/MyDrive/recherche_images_gallica/\"+row[\"img_path\"]).convert(\"RGB\")\n",
    "        images.append((image, str(row[\"sim\"]) +\" - \"+str(row[\"legend\"])))\n",
    "        \n",
    "    return images\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeff667-abbf-4bc2-a730-9270c8a934a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WebUI by Gradio\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Tab(\"Recherche\"):\n",
    "        with gr.Row():\n",
    "            corpus_select = gr.Dropdown([], label=\"Corpus\", info=\"Choisissez un corpus\", value=0)\n",
    "            update_btn = gr.Button(\"Rafraîchir la liste des corpus\")\n",
    "        with gr.Row():\n",
    "            search_type = gr.Dropdown([\"Texte & Image\", \"Image\", \"Texte\"], label=\"Type de recherche\", info=\"Choisissez un type de recherche\", value=0, interactive = False)\n",
    "            search_txt = gr.Textbox(label=\"Recherche\", info=\"Texte pour la recherche d'images\", interactive = False)\n",
    "        with gr.Row():\n",
    "            img_count = gr.Slider(1, 50, value=5, label=\"Nombre d'images\", info=\"Choisissez le nombre d'images à rechercher\", step=1, interactive = False)\n",
    "        with gr.Row():\n",
    "            search_btn = gr.Button(\"Rechercher des images\", interactive = False)\n",
    "        with gr.Row():\n",
    "            gallery = gr.Gallery(label=\"Generated images\", show_label=False, elem_id=\"gallery\", columns=[3], rows=[1], object_fit=\"fill\", height=\"auto\", interactive = False)       \n",
    "    with gr.Tab(\"Collecte\"):\n",
    "        coll_name = gr.Textbox(label=\"Nom de collection\", info=\"Entrez le nom de votre collection\", interactive = True)\n",
    "        search_ark = gr.Textbox(label=\"Arks\", info=\"Identifiants Arks à collecter\", interactive = True)\n",
    "        search_ark_btn = gr.Button(\"Lancer la collecte et les traitements\", interactive = True)\n",
    "        output_ark = gr.Textbox(label=\"Console\", info=\"Logs de la collecte\", interactive = False)\n",
    "        \n",
    "    search_btn.click(fn=deactivate, outputs=[corpus_select, update_btn, search_type, search_txt, img_count, search_btn, coll_name, search_ark, search_ark_btn]).then(search, inputs=[search_txt,search_type, img_count], outputs=[gallery]).then(fn=activate, outputs=[corpus_select, update_btn, search_type, search_txt, img_count, search_btn, coll_name, search_ark, search_ark_btn])\n",
    "    update_btn.click(fn=deactivate, outputs=[corpus_select, update_btn, search_type, search_txt, img_count, search_btn, coll_name, search_ark, search_ark_btn]).then(update_corpus, outputs=[corpus_select]).then(fn=activate, outputs=[corpus_select, update_btn, search_type, search_txt, img_count, search_btn, coll_name, search_ark, search_ark_btn])\n",
    "    search_ark_btn.click(fn=deactivate, outputs=[corpus_select, update_btn, search_type, search_txt, img_count, search_btn, coll_name, search_ark, search_ark_btn]).then(search_ark_fn, inputs=[search_ark, coll_name], outputs=[output_ark]).then(fn=activate, outputs=[corpus_select, update_btn, search_type, search_txt, img_count, search_btn, coll_name, search_ark, search_ark_btn])\n",
    "    corpus_select.change(fn=deactivate, outputs=[corpus_select, update_btn, search_type, search_txt, img_count, search_btn, coll_name, search_ark, search_ark_btn]).then(load_csv, inputs=[corpus_select], show_progress=True).then(fn=activate, outputs=[corpus_select, update_btn, search_type, search_txt, img_count, search_btn, coll_name, search_ark, search_ark_btn])\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
